{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neo4j.v1 import GraphDatabase, basic_auth\n",
    "import requests as r\n",
    "from requests.exceptions import *\n",
    "import time\n",
    "from lxml import html, etree\n",
    "import sys\n",
    "import dateparser\n",
    "import re\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We overwrite Jupyters defaut logging pipe and send all output to a the file *scraping.log*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='scraping.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize our database connection\n",
    "The results are stored in a NEO4J database. Make sure you have one, with the BOLT protocol enabled, running locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=basic_auth(\"neo4j\", \"pass\"))\n",
    "session = driver.session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create page and vote objects\n",
    "Scraping code is generally ugly, period. However, to create a bit more structure we define a few usefull classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptions\n",
    "Since scraping is pretty error prone due to the many HTTP request we define some exceptions to handle errors neathly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InvalidResponse(Exception):\n",
    "    def __init__(self, *args):\n",
    "        super(Exception, self).__init__(*args)\n",
    "        \n",
    "class ScraperException(Exception):\n",
    "    def __init__(self, *args):\n",
    "        super(Exception, self).__init__(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our general Scraper object will form the basis for each scraping operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Scraper(object):\n",
    "    def __init__(self, \n",
    "                 maxTries = 3,\n",
    "                 sleepTimeOnError = 30,\n",
    "                 accept404 = False):\n",
    "        self.maxTries = maxTries\n",
    "        self.sleepTime = sleepTimeOnError\n",
    "        self.logger = logging.getLogger('Scraper')\n",
    "        self.accept404 = accept404\n",
    "        \n",
    "    def scrape(self, url):\n",
    "        num = 0 #Set the repetition count to 0\n",
    "\n",
    "        while(True):\n",
    "            num += 1\n",
    "\n",
    "            if num >= self.maxTries:\n",
    "                raise ScraperException('Maximum number of tries exceeded.')\n",
    "\n",
    "            try:\n",
    "                resp = r.get(url)\n",
    "                if resp.status_code == 404 and self.accept404:\n",
    "                    return False\n",
    "                if resp.status_code != 200 or resp.content == None:\n",
    "                    raise InvalidResponse('Request was succesfull, but got no valid response.')\n",
    "                break\n",
    "            except RequestException:\n",
    "                # If we encounter an error of Requests, just retry immediately\n",
    "                self.logger.warning('Encountered a request exception. URL: {0} TRY: {1}'.format(url, num))\n",
    "                pass\n",
    "            except InvalidResponse as e:\n",
    "                # If the request was succesfull but we don't like the response\n",
    "                # sleep some time before retrying.\n",
    "                self.logger.warning('Encountered a Invalid Response. URL: {0} TRY: {1}'.format(url, num))\n",
    "                time.sleep(self.sleepTime)\n",
    "        return self.parse(resp)\n",
    "\n",
    "    def parse(self, response):\n",
    "        raise NotImplementedError('Parser method should be defined in an inherited class')\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PaginatedScraper to be used to iterate over content represented by multiple pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PaginatedScraper(Scraper):\n",
    "    def __init__(self, \n",
    "                 urlUpdate, \n",
    "                 timeBetween = 0, \n",
    "                 maxTries = 3, \n",
    "                 maxIterations = 0, \n",
    "                 sleepTimeOnError = 30):\n",
    "        super().__init__(maxTries,\n",
    "                        sleepTimeOnError)\n",
    "        self.urlUpdate = urlUpdate\n",
    "        self.timeBetween = timeBetween\n",
    "        self.maxIterations = maxIterations\n",
    "        self.logger = logging.getLogger(\"PaginatedScraper\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.iteration = 1\n",
    "        return self  \n",
    "    \n",
    "    def __next__(self):\n",
    "        # First call the urlUpdate closure to get the url for the next iteration\n",
    "        url = self.urlUpdate()\n",
    "        \n",
    "        # Sleep the desired time between updates\n",
    "        time.sleep(self.timeBetween)\n",
    "        \n",
    "        if self.maxIterations != 0 and self.iteration >= self.maxIterations:\n",
    "            raise StopIteration()       \n",
    "                         \n",
    "        try:\n",
    "            scrapedPage = super().scrape(url)\n",
    "        except ScraperException:\n",
    "            self.logger.error(\"Scraping failed even after retries. Skipping the URL. URL: {0}\".format(url))\n",
    "            return self.__next__()\n",
    "            \n",
    "        self.iteration += 1\n",
    "        # We may assume resp exists here and contains valid information\n",
    "        return scrapedPage\n",
    "    \n",
    "    def parse(self, response):\n",
    "        raise NotImplementedError('This class should not directly be initiated.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VotePagesScraper is a PaginatedScraper specific for the vote pages on tweedekamer.nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VotePagesScraper(PaginatedScraper):\n",
    "    BASE_URL = \"https://www.tweedekamer.nl/kamerstukken/\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def parse(self, response):\n",
    "        content = response.content\n",
    "        pageTree = html.fromstring(content)\n",
    "        pathString = '//li/div[@class=\"search-result-content\"]/h3/a/@href'\n",
    "        votes = pageTree.xpath(pathString)\n",
    "        \n",
    "        #If the current page is empty, stop scraping\n",
    "        if len(votes) < 1:\n",
    "            raise StopIteration()\n",
    "            \n",
    "        for vote in votes:\n",
    "            time.sleep(self.timeBetween)\n",
    "            yield VotePagesScraper.BASE_URL + vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document object will be used to parse documents related to Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Document(Scraper):\n",
    "    def parse(self, response):\n",
    "        if 'Content-Type' in response.headers and response.headers['Content-Type'] == 'text/xml':\n",
    "            tree = etree.fromstring(response.content)            \n",
    "            return {\n",
    "                \"text\": \"\\n\".join(tree.xpath('//vrije-tekst//al//text()')),\n",
    "                \"url\": response.url\n",
    "            }\n",
    "        return {\n",
    "            \"url\": response.url\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MetaData Object will be used to scrape the metadata for a specific vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MetaData(Scraper):\n",
    "    def parse(self, response):\n",
    "        result = []\n",
    "        \n",
    "        if 'Content-Type' in response.headers and response.headers['Content-Type'] == 'text/xml':\n",
    "            tree = etree.fromstring(response.content)\n",
    "            themes = tree.xpath('//metadata[@scheme=\"OVERHEID.TaxonomieBeleidsagenda\"]/@content')\n",
    "            \n",
    "            for theme in themes:\n",
    "                try:\n",
    "                    result.append({\n",
    "                            'major': theme.split(' | ')[0],\n",
    "                            'minor': theme.split(' | ')[1]\n",
    "                        })\n",
    "                except IndexError:\n",
    "                    continue                  \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vote object will be used to scrape multiple Votes in a VoteGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Vote(PaginatedScraper):\n",
    "    def parse(self, response):\n",
    "        content = response.content\n",
    "        if len(content) < 20:\n",
    "            raise StopIteration()\n",
    "        pageTree = html.fromstring(content)\n",
    "        medeLeden = pageTree.xpath('//dt[text()=\"Medeindiener\"]/following-sibling::dd[1]/a[1]/text()')\n",
    "        medePartijen = pageTree.xpath('//dt[text()=\"Medeindiener\"]/following-sibling::dd[1]/a[2]/text()')\n",
    "        medeIndieners = [{\"Lid\":a, \"Partij\": b} for a,b in zip(medeLeden,medePartijen)]\n",
    "        \n",
    "        return medeIndieners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VoteGroup object is a representation of one or multiple votes on the same topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VoteGroup(Scraper):\n",
    "    BASE_URL = \"https://www.tweedekamer.nl\"\n",
    "    def __init__(self, url, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.url = url\n",
    "        \n",
    "        self.i = 1\n",
    "        \n",
    "        self.content, self.urls = self.scrape(self.url)\n",
    "        self.logger = logging.getLogger(\"VoteGroupScraper\")\n",
    "        \n",
    "        \n",
    "    def getSubmitter(self, voteOverview):\n",
    "        lid = voteOverview.xpath('./div[@class=\"search-result-content\"]/p[@class=\"submitter\"]/a[1]/text()')\n",
    "        partij = voteOverview.xpath('./div[@class=\"search-result-content\"]/p[@class=\"submitter\"]/a[2]/text()')\n",
    "        \n",
    "        secretary = voteOverview.xpath('./div[@class=\"search-result-content\"]/p[@class=\"submitter\"]/text()')\n",
    "        \n",
    "        # Check whether the submitter is a parliament member\n",
    "        if len(lid) >0 and len(partij) > 0:\n",
    "            return {\n",
    "                \"Naam\": lid[0],\n",
    "                \"Partij\": partij[0],\n",
    "                \"Type\": \"Kamerlid\"\n",
    "            }\n",
    "        elif len(secretary) > 0 and re.match('([A-Za-z\\.\\s]*),([A-Za-z\\s)]*)', secretary[0]):\n",
    "            matches = re.match('([A-Za-z\\.\\s]*),([A-Za-z\\s)]*)', secretary[0]).groups()\n",
    "            # If it is not a member of parliament, try to check whether it is a secretary of state\n",
    "            return {\n",
    "                \"Naam\": matches[0].strip(' \\t\\n\\r'),\n",
    "                \"Partij\": \"None\",\n",
    "                \"Type\": matches[1].strip(' \\t\\n\\r')            \n",
    "            }\n",
    "        return False\n",
    "    \n",
    "    def getVoteResults(self, voteOverview):\n",
    "        resultsSection = voteOverview.xpath('./div[@class=\"vote-result\"]')\n",
    "        if len(resultsSection) > 0:\n",
    "            voteResults = []\n",
    "            \n",
    "            voteType = resultsSection[0].xpath('./p[@class=\"vote-type\"]/text')\n",
    "            resultRows = resultsSection[0].xpath('./table[@class=\"statistics\"]/tbody/tr')\n",
    "            for row in resultRows:\n",
    "                partij = row.xpath('./td[1]/a/text()')\n",
    "                zetels = row.xpath('./td[2]/text()')\n",
    "                voor = row.xpath('./td[3]/img/@width')\n",
    "                tegen = row.xpath('./td[4]/img/@width')\n",
    "                \n",
    "                if len(partij) > 0 and len(zetels) > 0 and (len(voor) > 0 or len(tegen) > 0):\n",
    "                    voor = int(voor[0]) if len(voor) > 0 else 0\n",
    "                    tegen = int(tegen[0]) if len(tegen) >0 else 0\n",
    "                    onthouding = int(zetels[0]) - voor - tegen  \n",
    "                    voteResults.append({\n",
    "                            \"partij\": partij[0],\n",
    "                            \"voor\": int(voor),\n",
    "                            \"tegen\": int(tegen),\n",
    "                            \"onthouding\": int(onthouding)\n",
    "                        })\n",
    "            return voteResults\n",
    "        return []\n",
    "                \n",
    "    \n",
    "    def getDocumentText(self, dossierNummer):\n",
    "        links = [\"https://zoek.officielebekendmakingen.nl/kst-{0}.xml\".format(dossierNummer)]\n",
    "        documents = []\n",
    "        documentScraper = Document()\n",
    "        \n",
    "        for link in links:\n",
    "            try:\n",
    "                document = documentScraper.scrape(link)\n",
    "                documents.append(document)\n",
    "            except Exception:\n",
    "                self.logger.exception(\"Could not parse document. Skipping it. URL: {0}\".format(link))\n",
    "                return []\n",
    "        return documents\n",
    "    \n",
    "    def getThemes(self, dossierNummer):\n",
    "        url = \"https://zoek.officielebekendmakingen.nl/kst-{0}/metadata.xml\".format(dossierNummer)\n",
    "        metaData = MetaData(accept404 = True)\n",
    "        \n",
    "        try:\n",
    "            result = metaData.scrape(url)\n",
    "        except ScraperException:\n",
    "            return []\n",
    "        return result\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.contentIter = iter(self.content)\n",
    "        \n",
    "        def urlIter():\n",
    "            urlIter = iter(self.urls)\n",
    "            def inner():\n",
    "                nonlocal urlIter\n",
    "                return VoteGroup.BASE_URL + next(urlIter)\n",
    "            return inner\n",
    "        \n",
    "        self.urlIter = urlIter\n",
    "        self.votesIter = iter(Vote(urlIter()))\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        voteOverview = next(self.contentIter)\n",
    "        medeIndieners = []\n",
    "        try:\n",
    "            medeIndieners = next(self.votesIter)\n",
    "        except StopIteration:\n",
    "            self.logger.error(\"Vote seems to have no detail page, skipping it.\")\n",
    "            return self.__next__()\n",
    "        except ScraperException:\n",
    "            self.logger.error(\"Scraper Exception, skipping the vote.\")\n",
    "            return self.__next__()\n",
    "\n",
    "            \n",
    "        try:\n",
    "            dossierNummer = voteOverview.xpath('./div[@class=\"search-result-properties\"]/p[1]/text()')[0]\n",
    "            data = {\n",
    "                \"group\": self.groupProperties,\n",
    "                \"category\": voteOverview.xpath('./p[@class=\"search-result-category\"]/text()')[0],\n",
    "                \"title\": voteOverview.xpath('./div[@class=\"search-result-content\"]/h3/a[1]/text()')[0]\n",
    "                    .replace('\\n', '')\n",
    "                    .replace('\\t', '')\n",
    "                    .replace('  ', ''),\n",
    "                \"indiener\": self.getSubmitter(voteOverview), \n",
    "                \"medeIndieners\": medeIndieners,\n",
    "                \"datum\": \n",
    "                    int(dateparser.parse(\n",
    "                        voteOverview.xpath('./div[@class=\"search-result-properties\"]/p[@class=\"date\"]/text()')[0],\n",
    "                        settings={'TIMEZONE': 'EST'}\n",
    "                    ).timestamp()),\n",
    "                \"dossierNummer\": dossierNummer,\n",
    "                \"documents\": self.getDocumentText(dossierNummer),\n",
    "                \"besluit\": voteOverview.xpath('./div[@class=\"search-result-content\"]/p[@class=\"result\"]/span/text()')[0]\n",
    "                    .replace('.', ''),\n",
    "                \"voteResults\": self.getVoteResults(voteOverview),\n",
    "                \"themes\": self.getThemes(dossierNummer)\n",
    "            }\n",
    "        except IndexError:\n",
    "            # If an index error is raised we can't parse a vote in a meaningfull way. We decide to skip it.\n",
    "            self.logger.error(\"Index Error for vote, skipping the vote.\")\n",
    "            return self.__next__()\n",
    "        return data\n",
    "        \n",
    "    def parse(self, response):\n",
    "        content=response.content\n",
    "        pageTree = html.fromstring(content)\n",
    "        \n",
    "        try:\n",
    "            groupTitle = pageTree.xpath('//div[@class=\"center-content\"]/h1/text()')[0].strip(' \\t\\n\\r')\n",
    "            groupLocation = pageTree.xpath('//div[@class=\"center-content\"]/p[@class=\"vote-info\"]/text()')[0].strip(' \\t\\n\\r')\n",
    "            groupDate = int(dateparser.parse(\n",
    "                pageTree.xpath('//div[@class=\"center-content\"]/p[@class=\"vote-info\"]\\\n",
    "/span[@class=\"date\"]/text()')[0],\n",
    "                settings={'TIMEZONE': 'EST'}\n",
    "            ).timestamp())\n",
    "        except IndexError:\n",
    "            self.logger.error(\"Index error for votegroup, skipping it.\")\n",
    "            raise StopIteration()\n",
    "            \n",
    "        votes = pageTree.xpath('//div[@class=\"vote-results\"]/ul/li')\n",
    "        urls = pageTree.xpath('//div[@class=\"vote-results\"]/ul/li\\\n",
    "/div[@class=\"search-result-content\"]/h3/a/@href')\n",
    "                              \n",
    "        self.numberOfVotes = len(votes)\n",
    "        \n",
    "        self.groupProperties = {\n",
    "            \"title\": groupTitle\n",
    "                    .replace('\\n', '')\n",
    "                    .replace('\\t', '')\n",
    "                    .replace('  ', ''),\n",
    "            \"location\": groupLocation,\n",
    "            \"date\": groupDate,\n",
    "            \"numberOfItems\": self.numberOfVotes\n",
    "        }        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return votes, urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To loop through the desired URLs we use a closure that returns the next URL every time it's called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def votePagesUrl():\n",
    "    currentPage = 0\n",
    "    def calculateUrl():\n",
    "        nonlocal currentPage\n",
    "        url = \"https://www.tweedekamer.nl/kamerstukken/stemmingsuitslagen?\\\n",
    "qry=%2A&fld_tk_categorie=Kamerstukken&\\\n",
    "fld_tk_subcategorie=Stemmingsuitslagen&\\\n",
    "srt=date%3Adesc%3Adate%2Cprl_volgorde%3Aasc%3Anum&\\\n",
    "clusterName=Stemmingsuitslagen&\\\n",
    "Type=Kamerstukken&\\\n",
    "fromdate=01%2F01%2F2016&\\\n",
    "todate=31%2F12%2F2016&\\\n",
    "dpp=15&sta={0}\".format(currentPage)\n",
    "        currentPage += 15\n",
    "        return url\n",
    "    \n",
    "    return calculateUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepareQuery(vote):\n",
    "    query = \"\"\n",
    "    \n",
    "    # Add the location\n",
    "    query += \"MERGE (location:Location {{name:'{0[group][location]}'}})\\n\".format(vote)\n",
    "    \n",
    "    # Add the overlaying topic\n",
    "    query += (\"MERGE (topic:Topic{{\"\n",
    "                 \"name: '{0[group][title]}'\"\n",
    "                 \"}})\\n\").format(vote)\n",
    "    \n",
    "    query += \"MERGE (location)<-[discussed:Discussed {{date: {0[group][date]}}}]-(topic)\\n\".format(vote)\n",
    "    \n",
    "    # Add the vote\n",
    "    query += (\"CREATE (vote:Vote {{\"\n",
    "             \"title: '{0[title]}',\"\n",
    "             \"id: '{0[dossierNummer]}',\" \n",
    "             \"ruling: '{0[besluit]}',\"\n",
    "             \"submitted: {0[datum]}\"\n",
    "             \"}})\\n\").format(vote)\n",
    "    \n",
    "    #Add the relationship to the overlaying topic\n",
    "    query += \"CREATE (vote)-[:PartOf]->(topic)\\n\"\n",
    "      \n",
    "    if vote['indiener']:\n",
    "        # We add the submitter\n",
    "        query += (\"MERGE (submitter:Person {{\"\n",
    "                    \"name: '{0[indiener][Naam]}',\"\n",
    "                    \"function: '{0[indiener][Type]}'\"\n",
    "                    \"}})\\n\").format(vote)\n",
    "\n",
    "        #Add the relation to the vote\n",
    "        query += (\"CREATE (submitter)-[:Submitted {{ date: {0[datum]} }}]->(vote)\\n\").format(vote)\n",
    "    \n",
    "        # The party of the submiter\n",
    "        query += (\"MERGE (party:Party {{ name:'{0[indiener][Partij]}' }})\\n\").format(vote)\n",
    "    \n",
    "        #The membership of the submitter to the party\n",
    "        query += (\"MERGE (submitter)-[:Member]->(party)\\n\")\n",
    "    \n",
    "    # loop through each co-submitter\n",
    "    query += (\"WITH \"+listOfDictsToString(vote['medeIndieners'])+ \" AS cosubmitters\\n\")\n",
    "    #Foreach loop\n",
    "    query += (\"FOREACH (pers IN cosubmitters | \\n\"\n",
    "             \"MERGE (vote:Vote {{\"\n",
    "             \"id: '{0[dossierNummer]}',\" \n",
    "             \"ruling: '{0[besluit]}'\"\n",
    "             \"}})\\n\"\n",
    "             \"MERGE (cosub:Person {{\"\n",
    "             \"name: pers.Lid,\"\n",
    "             \"function: 'Kamerlid'\"\n",
    "             \"}})\\n\"\n",
    "             \"MERGE (cosubParty:Party {{name: pers.Partij}})\\n\"\n",
    "             \"MERGE (cosub)-[:Member]->(cosubParty)\\n\"\n",
    "             \"CREATE (cosub)-[:Cosubmitted]->(vote)\\n\"\n",
    "             \")\\n\").format(vote)\n",
    "    \n",
    "    # loop through each theme\n",
    "    query += (\"WITH \"+listOfDictsToString(vote['themes'])+ \" AS themes\\n\")\n",
    "    #Foreach loop\n",
    "    query += (\"FOREACH (theme IN themes | \\n\"\n",
    "             \"MERGE (vote:Vote {{\"\n",
    "             \"id: '{0[dossierNummer]}',\" \n",
    "             \"ruling: '{0[besluit]}'\"\n",
    "             \"}})\\n\"\n",
    "             \"MERGE (mat:MajorTheme {{\"\n",
    "             \"name: theme.major\"\n",
    "             \"}})\\n\"\n",
    "             \"MERGE (mit:MinorTheme {{\"\n",
    "             \"name: theme.minor\"\n",
    "             \"}})\\n\"\n",
    "             \"MERGE (mat)<-[:ParentTheme]-(mit)\\n\"\n",
    "             \"CREATE (mit)<-[:Theme]-(vote)\\n\"\n",
    "             \")\\n\").format(vote)\n",
    "    \n",
    "    # Vote results\n",
    "    query += (\"WITH \"+listOfDictsToString(vote['voteResults'])+ \" AS voters\\n\")\n",
    "    \n",
    "    #Add vote edges\n",
    "    query += (\"FOREACH (voteRes IN voters | \\n\"\n",
    "             \"MERGE (vote:Vote {{\"\n",
    "             \"id: '{0[dossierNummer]}',\" \n",
    "             \"ruling: '{0[besluit]}'\"\n",
    "             \"}})\\n\"\n",
    "             \"MERGE (voter:Party {{name: voteRes.partij}})\\n\"\n",
    "             \"CREATE (voter)-[:Voted {{\"\n",
    "             \"pro:voteRes.voor, \"\n",
    "             \"con:voteRes.tegen, \"\n",
    "             \"withold:voteRes.onthouding\"\n",
    "             \"}}]->(vote)\\n\"\n",
    "             \")\").format(vote)\n",
    "    \n",
    "    # We add the documents to the vote\n",
    "    query += (\"WITH \"+listOfDictsToString(vote['documents'])+ \" AS docs\\n\")\n",
    "    #Foreach loop\n",
    "    query += (\"FOREACH (doc IN docs | \\n\"\n",
    "             \"MERGE (vote:Vote {{\"\n",
    "             \"id: '{0[dossierNummer]}',\" \n",
    "             \"ruling: '{0[besluit]}'\"\n",
    "             \"}})\\n\"\n",
    "             \"CREATE (accompDoc:Document {{\"\n",
    "             \"url: doc.url,\"\n",
    "             \"text: doc.text\"\n",
    "             \"}})\\n\"\n",
    "             \"CREATE (accompDoc)-[:Describes]->(vote)\\n\"\n",
    "             \")\\n\").format(vote)\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We cannot, unfortunately, just use str(), due to the quotation around the keys that will add\n",
    "# The following implementation is crappy, non-pythonic and to be used at your own risk:)\n",
    "\n",
    "def listOfDictsToString(listOfDictionaries):\n",
    "    value = \"[\"\n",
    "    outerIt =1\n",
    "    outerSize = len(listOfDictionaries)\n",
    "    \n",
    "    for item in listOfDictionaries:\n",
    "        value = value + \"{\"\n",
    "        size = len(item)\n",
    "        iteration = 1\n",
    "        \n",
    "        for key, val in item.items():\n",
    "            if isinstance(val, str):\n",
    "                value = value + key + \":\" + \"'{0}'\".format(val)\n",
    "            else:\n",
    "                value = value + key + \":\" + \"{0}\".format(val)\n",
    "            if iteration != size:\n",
    "                value = value + \",\"\n",
    "            iteration +=1\n",
    "        value = value + \"}\"\n",
    "        \n",
    "        if outerIt != outerSize:\n",
    "            value = value + \",\"\n",
    "        \n",
    "        outerIt +=1\n",
    "    \n",
    "    value = value + \"]\"\n",
    "    return value            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def addEscapeSequences(data):\n",
    "    if type(data) == dict:\n",
    "        for key, value in data.items():\n",
    "            data[key] = addEscapeSequences(value)\n",
    "                \n",
    "    elif type(data) == list:\n",
    "        for key, value in enumerate(data):\n",
    "            data[key] = addEscapeSequences(value)\n",
    "    \n",
    "    elif isinstance(data, str):\n",
    "        data = data.translate(str.maketrans({\"'\":  r\"\\'\"}))           \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "a=None\n",
    "c=None\n",
    "for voteListPage in VotePagesScraper(votePagesUrl(), maxIterations = 1000000):\n",
    "    for voteGroupUrl in voteListPage:\n",
    "        try:\n",
    "            for vote in VoteGroup(voteGroupUrl):\n",
    "                a=session.run(prepareQuery(addEscapeSequences(vote)))\n",
    "                for b in a:\n",
    "                    print(b)\n",
    "                sys.stdout.write(\"\\rDoing thing {0}\".format(k))\n",
    "                k=k+1\n",
    "        except Exception:\n",
    "            logging.exception(\"Something general went wrong.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
